{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fbd644",
   "metadata": {},
   "source": [
    "# TrackNet Tennis Ball Tracking Inference\n",
    "\n",
    "This notebook performs tennis ball tracking inference using a pre-trained TrackNet model. It processes video clips to detect and track tennis ball positions, generating both prediction data and visualization videos.\n",
    "\n",
    "## Overview\n",
    "- Load pre-trained TrackNet model\n",
    "- Process video frames to detect ball positions\n",
    "- Generate tracking predictions and trajectory visualizations\n",
    "- Export results to CSV and video files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7f174",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for deep learning, computer vision, and data processing:\n",
    "- **PyTorch**: For neural network model loading and inference\n",
    "- **OpenCV**: For video processing and image manipulation\n",
    "- **NumPy**: For numerical computations\n",
    "- **Pandas**: For data handling and CSV export\n",
    "- **Collections**: For efficient frame buffering\n",
    "- **tqdm**: For progress bars during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f38067",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Configure all inference parameters including:\n",
    "- **Model paths**: Location of pre-trained TrackNet weights\n",
    "- **Input video**: Source video file for ball tracking\n",
    "- **Model dimensions**: Input resolution (640x360) for processing\n",
    "- **Output settings**: Paths for generated videos and CSV predictions\n",
    "- **Visualization parameters**: Colors, sizes, and thickness for ball detection display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e59d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: models\\model_best.pt\n",
      "Processing video clip starting at: videos\\Clip1.mp4\n",
      "Output will be saved to: videos\\Clip1_tracknet_track.mp4\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig:\n",
    "    # Path Configuration\n",
    "    MODEL_PATH = Path('models/model_best.pt')\n",
    "    VIDEO_CLIP_PATH = Path('videos/Clip1.mp4') \n",
    "\n",
    "    # Model & Input\n",
    "    INPUT_WIDTH = 640\n",
    "    INPUT_HEIGHT = 360\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Visualization Configuration \n",
    "    OUTPUT_VIDEO_PATH = Path('videos/Clip1_tracknet_track.mp4')\n",
    "    OUTPUT_TRACKING_VIDEO_PATH = Path('videos/Clip1_tracknet_trajectory.mp4')\n",
    "    OUTPUT_CSV_PATH = Path('clip1_tracknet_predictions.csv')\n",
    "    CIRCLE_COLOR = (0, 0, 255)  \n",
    "    CIRCLE_RADIUS = 5\n",
    "    CIRCLE_THICKNESS = 3\n",
    "\n",
    "    # Trajectory trail\n",
    "    TRAIL_COLOR = (0, 0, 255) \n",
    "    TRAIL_THICKNESS = 3\n",
    "\n",
    "# Create an instance of the config\n",
    "config = InferenceConfig()\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print(f\"Loading model from: {config.MODEL_PATH}\")\n",
    "print(f\"Processing video clip starting at: {config.VIDEO_CLIP_PATH}\")\n",
    "print(f\"Output will be saved to: {config.OUTPUT_VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54303d4c",
   "metadata": {},
   "source": [
    "## TrackNet Model Architecture\n",
    "\n",
    "Define the TrackNet neural network architecture and helper functions:\n",
    "\n",
    "### Components:\n",
    "- **ConvBlock**: Basic convolutional block with Conv2D, ReLU, and BatchNorm\n",
    "- **TrackNet**: Complete encoder-decoder architecture\n",
    "  - **Encoder**: VGG16-style convolutional layers with max pooling\n",
    "  - **Decoder**: Upsampling layers to reconstruct heatmap predictions\n",
    "- **postprocess()**: Converts model heatmap output to ball coordinates using circle detection\n",
    "\n",
    "The model takes 3 consecutive frames (9 channels) as input and outputs a heatmap indicating ball position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block with Conv2D + ReLU + BatchNorm.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        kernel_size (int): Convolution kernel size (default: 3)\n",
    "        pad (int): Padding size (default: 1)\n",
    "        stride (int): Convolution stride (default: 1)\n",
    "        bias (bool): Whether to use bias in convolution (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class TrackNet(nn.Module):\n",
    "    \"\"\"\n",
    "    TrackNet model for tennis ball tracking using encoder-decoder architecture.\n",
    "    \n",
    "    Takes 3 consecutive frames (9 channels) as input and outputs a 256-class\n",
    "    heatmap for ball localization. Architecture follows VGG16-style encoder\n",
    "    with DeconvNet-style decoder for pixel-level classification.\n",
    "    \n",
    "    Args:\n",
    "        out_channels (int): Number of output classes (default: 256)\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Encoder: VGG16-style feature extraction\n",
    "        # Block 1: 9 -> 64 channels, spatial size: 640x360 -> 320x180\n",
    "        self.conv1 = ConvBlock(in_channels=9, out_channels=64)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 2: 64 -> 128 channels, spatial size: 320x180 -> 160x90\n",
    "        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 3: 128 -> 256 channels, spatial size: 160x90 -> 80x45\n",
    "        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 4: 256 -> 512 channels (bottleneck), spatial size: 80x45\n",
    "        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n",
    "        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n",
    "\n",
    "        # Decoder: DeconvNet-style upsampling\n",
    "        # Upsample 1: 80x45 -> 160x90, 512 -> 256 channels\n",
    "        self.ups1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n",
    "        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.conv13 = ConvBlock(in_channels=256, out_channels=256)\n",
    "\n",
    "        # Upsample 2: 160x90 -> 320x180, 256 -> 128 channels\n",
    "        self.ups2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv14 = ConvBlock(in_channels=256, out_channels=128)\n",
    "        self.conv15 = ConvBlock(in_channels=128, out_channels=128)\n",
    "\n",
    "        # Upsample 3: 320x180 -> 640x360, 128 -> 64 -> out_channels\n",
    "        self.ups3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv16 = ConvBlock(in_channels=128, out_channels=64)\n",
    "        self.conv17 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.conv18 = ConvBlock(in_channels=64, out_channels=self.out_channels)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self._init_weights()\n",
    "                  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through TrackNet.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, 9, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output heatmap of shape (N, 256, H, W)\n",
    "        \"\"\"\n",
    "        # Encoder path\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)    \n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.ups1(x)\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.ups2(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        x = self.ups3(x)\n",
    "        x = self.conv16(x)\n",
    "        x = self.conv17(x)\n",
    "        x = self.conv18(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.uniform_(module.weight, -0.05, 0.05)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)    \n",
    "    \n",
    "def postprocess(heatmap):\n",
    "    \"\"\"\n",
    "    MODIFIED to return center_x, center_y, and radius.\n",
    "    \"\"\"\n",
    "    heatmap = heatmap.astype(np.uint8)\n",
    "    ret, binary_heatmap = cv2.threshold(heatmap, 127, 255, cv2.THRESH_BINARY)\n",
    "    circles = cv2.HoughCircles(binary_heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=1,\n",
    "                               param1=50, param2=2, minRadius=2, maxRadius=7)\n",
    "    \n",
    "    if circles is not None and len(circles) == 1:\n",
    "        x = circles[0][0][0]\n",
    "        y = circles[0][0][1]\n",
    "        r = circles[0][0][2]\n",
    "        return x, y, r\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "print(\"Model and helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72640387",
   "metadata": {},
   "source": [
    "## Model Loading and Initialization\n",
    "\n",
    "Load the pre-trained TrackNet model:\n",
    "1. **Instantiate** the TrackNet architecture\n",
    "2. **Load weights** from the saved model file (`model_best.pt`)\n",
    "3. **Set evaluation mode** to disable training-specific behaviors (dropout, batch norm updates)\n",
    "4. **Error handling** to ensure model file exists before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01481d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully!\n",
      "Model is in evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = TrackNet().to(config.DEVICE)\n",
    "\n",
    "# Load the Best Model's Weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH, map_location=config.DEVICE))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {config.MODEL_PATH}. Please check the path in the config.\")\n",
    "    # Stop execution if model not found\n",
    "    assert False\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model is in evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67dec43",
   "metadata": {},
   "source": [
    "## Video Processing and Prediction Generation\n",
    "\n",
    "Process the input video to generate ball position predictions:\n",
    "\n",
    "### Process:\n",
    "1. **Open video** and extract metadata (frames, dimensions)\n",
    "2. **Frame buffering**: Maintain sliding window of 3 consecutive frames\n",
    "3. **Model inference**: Process frame triplets through TrackNet\n",
    "4. **Coordinate extraction**: Convert heatmap predictions to ball positions\n",
    "5. **Scale conversion**: Transform coordinates from model input size to original video dimensions\n",
    "6. **Data collection**: Store predictions in structured format for CSV export\n",
    "\n",
    "### Output:\n",
    "- CSV file with frame-by-frame ball coordinates\n",
    "- Bounding box format compatible with tracking evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dba259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: 207 frames, 1280x720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 207/207 [00:06<00:00, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Generation Complete ---\n",
      "Predictions saved to: clip1_tracknet_predictions.csv\n",
      "\n",
      "First 10 rows of the prediction data:\n",
      "   frame_id  tracknet_x  tracknet_y  tracknet_w  tracknet_h\n",
      "0         2       594.0       384.0        20.0        20.0\n",
      "1         3       594.0       364.0        20.0        20.0\n",
      "2         4       592.0       348.0        20.0        20.0\n",
      "3         5       590.0       332.0        20.0        20.0\n",
      "4         6       590.0       316.0        20.0        20.0\n",
      "5         7       590.0       304.0        20.0        20.0\n",
      "6         8       590.0       294.0        20.0        20.0\n",
      "7         9       588.0       284.0        20.0        20.0\n",
      "8        10       586.0       272.0        20.0        20.0\n",
      "9        11       586.0       266.0        20.0        20.0\n",
      "\n",
      "Detected a ball in 186 out of 207 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize video capture and validate file\n",
    "cap = cv2.VideoCapture(str(config.VIDEO_CLIP_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file at {config.VIDEO_CLIP_PATH}\")\n",
    "    assert False\n",
    "\n",
    "# Extract video properties\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize frame buffer and prediction storage\n",
    "frame_buffer = collections.deque(maxlen=3)  # Rolling window of 3 frames\n",
    "predictions_list = []\n",
    "frame_id_counter = 0\n",
    "\n",
    "print(f\"\\nProcessing video: {total_frames} frames, {original_width}x{original_height}\")\n",
    "pbar = tqdm(total=total_frames, desc=\"Generating Predictions\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize frame to model input dimensions\n",
    "    resized_frame = cv2.resize(frame, (config.INPUT_WIDTH, config.INPUT_HEIGHT))\n",
    "    frame_buffer.append(resized_frame)\n",
    "    \n",
    "    # Process when we have 3 consecutive frames\n",
    "    if len(frame_buffer) == 3:\n",
    "        # Convert frames to tensor format: BGR->RGB, normalize, stack channels\n",
    "        imgs_list = [torch.from_numpy(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)).float() / 255.0 for f in frame_buffer]\n",
    "        input_tensor = torch.cat([t.permute(2, 0, 1) for t in imgs_list], dim=0).unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        \n",
    "        # Extract ball coordinates from heatmap\n",
    "        pred_heatmap = torch.argmax(predictions.squeeze(0), dim=0).cpu().numpy()\n",
    "        px, py, pr = postprocess(pred_heatmap)\n",
    "        \n",
    "        if px is not None:\n",
    "            # Scale coordinates to original video dimensions\n",
    "            scale_x = original_width / config.INPUT_WIDTH\n",
    "            scale_y = original_height / config.INPUT_HEIGHT\n",
    "            \n",
    "            center_x = px * scale_x\n",
    "            center_y = py * scale_y\n",
    "            radius = pr * ((scale_x + scale_y) / 2.0)\n",
    "            \n",
    "            # Convert to bounding box format for tracking evaluation\n",
    "            tracknet_x = center_x - radius\n",
    "            tracknet_y = center_y - radius\n",
    "            tracknet_w = 20  # Fixed bounding box size\n",
    "            tracknet_h = 20\n",
    "            \n",
    "            predictions_list.append({\n",
    "                'frame_id': frame_id_counter,\n",
    "                'tracknet_x': int(tracknet_x),\n",
    "                'tracknet_y': int(tracknet_y),\n",
    "                'tracknet_w': int(tracknet_w),\n",
    "                'tracknet_h': int(tracknet_h)\n",
    "            })\n",
    "        else:\n",
    "            # Record NaN for undetected ball\n",
    "            predictions_list.append({\n",
    "                'frame_id': frame_id_counter,\n",
    "                'tracknet_x': np.nan,\n",
    "                'tracknet_y': np.nan,\n",
    "                'tracknet_w': np.nan,\n",
    "                'tracknet_h': np.nan\n",
    "            })\n",
    "            \n",
    "    frame_id_counter += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "# Clean up resources\n",
    "pbar.close()\n",
    "cap.release()\n",
    "\n",
    "# Export predictions to CSV\n",
    "tracknet_df = pd.DataFrame(predictions_list)\n",
    "tracknet_df.to_csv(config.OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"\\n--- Prediction Generation Complete ---\")\n",
    "print(f\"Predictions saved to: {config.OUTPUT_CSV_PATH}\")\n",
    "print(\"\\nFirst 10 rows of the prediction data:\")\n",
    "print(tracknet_df.head(10))\n",
    "\n",
    "# Summary statistics\n",
    "detected_frames = tracknet_df['tracknet_x'].notna().sum()\n",
    "print(f\"\\nDetected a ball in {detected_frames} out of {total_frames} frames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21bdcc",
   "metadata": {},
   "source": [
    "## Trajectory Visualization Generation\n",
    "\n",
    "Create an annotated video showing ball tracking results:\n",
    "\n",
    "### Features:\n",
    "1. **Real-time detection**: Process each frame and detect ball position\n",
    "2. **Trajectory trail**: Draw connected lines showing ball's path over time\n",
    "3. **Current position**: Highlight current frame's detected ball location\n",
    "4. **Video output**: Generate MP4 with overlaid tracking visualizations\n",
    "\n",
    "### Visualization Elements:\n",
    "- **Red circles**: Current ball position\n",
    "- **Red trajectory lines**: Historical ball path\n",
    "- **Progress tracking**: Real-time processing status\n",
    "\n",
    "This creates a comprehensive visual representation of the tracking performance for analysis and presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cf3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: 207 frames, 30 fps, 1280x720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|██████████| 207/207 [00:06<00:00, 30.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Trajectory video saved to: videos\\Clip1_tracknet_trajectory.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize video capture for trajectory visualization\n",
    "cap = cv2.VideoCapture(str(config.VIDEO_CLIP_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file at {config.VIDEO_CLIP_PATH}\")\n",
    "    assert False\n",
    "\n",
    "# Extract video properties\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Setup video writer for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(str(config.OUTPUT_TRACKING_VIDEO_PATH), fourcc, fps, (original_width, original_height))\n",
    "\n",
    "# Initialize processing variables\n",
    "frame_buffer = collections.deque(maxlen=3)  # Rolling window of 3 frames\n",
    "trajectory_points = []  # Store all detected ball positions for trail\n",
    "\n",
    "print(f\"\\nProcessing video: {total_frames} frames, {fps} fps, {original_width}x{original_height}\")\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame for model input\n",
    "    resized_frame = cv2.resize(frame, (config.INPUT_WIDTH, config.INPUT_HEIGHT))\n",
    "    frame_buffer.append(resized_frame)\n",
    "    \n",
    "    current_ball_pos = None  # Reset for current frame\n",
    "    \n",
    "    # Process when we have 3 consecutive frames\n",
    "    if len(frame_buffer) == 3:\n",
    "        # Convert frames to tensor format: BGR->RGB, normalize, stack channels\n",
    "        imgs_list = [torch.from_numpy(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)).float() / 255.0 for f in frame_buffer]\n",
    "        input_tensor = torch.cat([t.permute(2, 0, 1) for t in imgs_list], dim=0).unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        \n",
    "        # Extract ball coordinates from heatmap\n",
    "        pred_heatmap = torch.argmax(predictions.squeeze(0), dim=0).cpu().numpy()\n",
    "        px, py = postprocess(pred_heatmap)\n",
    "        \n",
    "        if px is not None:\n",
    "            # Scale coordinates to original video dimensions\n",
    "            original_x = int(px * (original_width / config.INPUT_WIDTH))\n",
    "            original_y = int(py * (original_height / config.INPUT_HEIGHT))\n",
    "            current_ball_pos = (original_x, original_y)\n",
    "            trajectory_points.append(current_ball_pos)\n",
    "            \n",
    "    # Draw trajectory trail as connected lines\n",
    "    if len(trajectory_points) > 1:\n",
    "        for i in range(1, len(trajectory_points)):\n",
    "            cv2.line(frame, trajectory_points[i - 1], trajectory_points[i], config.TRAIL_COLOR, config.TRAIL_THICKNESS)\n",
    "            \n",
    "    # Draw current ball position\n",
    "    if current_ball_pos is not None:\n",
    "        cv2.circle(frame, current_ball_pos, config.CIRCLE_RADIUS, config.CIRCLE_COLOR, config.CIRCLE_THICKNESS)\n",
    "\n",
    "    video_writer.write(frame)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Clean up resources\n",
    "pbar.close()\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "print(f\"\\n--- Inference Complete ---\")\n",
    "print(f\"Trajectory video saved to: {config.OUTPUT_TRACKING_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
