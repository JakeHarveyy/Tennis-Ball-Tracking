{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e59d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: models\\model_best.pt\n",
      "Processing video clip starting at: videos\\Clip1.mp4\n",
      "Output will be saved to: videos\\Clip1_tracknet_track.mp4\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig:\n",
    "    # --- Paths ---\n",
    "    MODEL_PATH = Path('models/model_best.pt')\n",
    "\n",
    "    # IMPORTANT: Update this path to the VIDEO you want to process\n",
    "    # Let's pick a clip from the validation set (e.g., game8)\n",
    "    VIDEO_CLIP_PATH = Path('videos/Clip1.mp4') \n",
    "\n",
    "    # --- Model & Input ---\n",
    "    INPUT_WIDTH = 640\n",
    "    INPUT_HEIGHT = 360\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Visualization ---\n",
    "    OUTPUT_VIDEO_PATH = Path('videos/Clip1_tracknet_track.mp4')\n",
    "    OUTPUT_TRACKING_VIDEO_PATH = Path('videos/Clip1_tracknet_trajectory.mp4')\n",
    "    OUTPUT_CSV_PATH = Path('clip1_tracknet_predictions.csv')\n",
    "    CIRCLE_COLOR = (0, 0, 255)  \n",
    "    CIRCLE_RADIUS = 5\n",
    "    CIRCLE_THICKNESS = 3\n",
    "\n",
    "    # Trajectory trail\n",
    "    TRAIL_COLOR = (0, 0, 255) \n",
    "    TRAIL_THICKNESS = 3\n",
    "\n",
    "# Create an instance of the config\n",
    "config = InferenceConfig()\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print(f\"Loading model from: {config.MODEL_PATH}\")\n",
    "print(f\"Processing video clip starting at: {config.VIDEO_CLIP_PATH}\")\n",
    "print(f\"Output will be saved to: {config.OUTPUT_VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a787fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class TrackNet(nn.Module):\n",
    "    def __init__(self, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # --- Encoder (VGG16 Style) ---\n",
    "        self.conv1 = ConvBlock(in_channels=9, out_channels=64)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n",
    "        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n",
    "\n",
    "        # -- Decoder (DeconvNet Style) ---\n",
    "        self.ups1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n",
    "        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.conv13 = ConvBlock(in_channels=256, out_channels=256)\n",
    "\n",
    "        self.ups2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv14 = ConvBlock(in_channels=256, out_channels=128)\n",
    "        self.conv15 = ConvBlock(in_channels=128, out_channels=128)\n",
    "\n",
    "        self.ups3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv16 = ConvBlock(in_channels=128, out_channels=64)\n",
    "        self.conv17 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.conv18 = ConvBlock(in_channels=64, out_channels=self.out_channels)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self._init_weights()\n",
    "                  \n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)    \n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.ups1(x)\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.ups2(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        x = self.ups3(x)\n",
    "        x = self.conv16(x)\n",
    "        x = self.conv17(x)\n",
    "        x = self.conv18(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.uniform_(module.weight, -0.05, 0.05)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)    \n",
    "    \n",
    "def postprocess(heatmap):\n",
    "    \"\"\"\n",
    "    MODIFIED to return center_x, center_y, and radius.\n",
    "    \"\"\"\n",
    "    heatmap = heatmap.astype(np.uint8)\n",
    "    ret, binary_heatmap = cv2.threshold(heatmap, 127, 255, cv2.THRESH_BINARY)\n",
    "    circles = cv2.HoughCircles(binary_heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=1,\n",
    "                               param1=50, param2=2, minRadius=2, maxRadius=7)\n",
    "    \n",
    "    if circles is not None and len(circles) == 1:\n",
    "        x = circles[0][0][0]\n",
    "        y = circles[0][0][1]\n",
    "        r = circles[0][0][2]\n",
    "        return x, y, r\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "print(\"Model and helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01481d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully!\n",
      "Model is in evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = TrackNet().to(config.DEVICE)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "try:\n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH, map_location=config.DEVICE))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {config.MODEL_PATH}. Please check the path in the config.\")\n",
    "    # Stop execution if model not found\n",
    "    assert False\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model is in evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4dba259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: 207 frames, 1280x720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 207/207 [00:06<00:00, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Generation Complete ---\n",
      "Predictions saved to: clip1_tracknet_predictions.csv\n",
      "\n",
      "First 10 rows of the prediction data:\n",
      "   frame_id  tracknet_x  tracknet_y  tracknet_w  tracknet_h\n",
      "0         2       594.0       384.0        20.0        20.0\n",
      "1         3       594.0       364.0        20.0        20.0\n",
      "2         4       592.0       348.0        20.0        20.0\n",
      "3         5       590.0       332.0        20.0        20.0\n",
      "4         6       590.0       316.0        20.0        20.0\n",
      "5         7       590.0       304.0        20.0        20.0\n",
      "6         8       590.0       294.0        20.0        20.0\n",
      "7         9       588.0       284.0        20.0        20.0\n",
      "8        10       586.0       272.0        20.0        20.0\n",
      "9        11       586.0       266.0        20.0        20.0\n",
      "\n",
      "Detected a ball in 186 out of 207 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# In[4]\n",
    "# =============================================================================\n",
    "# Step 3: Run Inference and Save Results to CSV\n",
    "# =============================================================================\n",
    "\n",
    "cap = cv2.VideoCapture(str(config.VIDEO_CLIP_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file at {config.VIDEO_CLIP_PATH}\")\n",
    "    assert False\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frame_buffer = collections.deque(maxlen=3)\n",
    "# *** List to store prediction data for each frame ***\n",
    "predictions_list = []\n",
    "frame_id_counter = 0\n",
    "\n",
    "print(f\"\\nProcessing video: {total_frames} frames, {original_width}x{original_height}\")\n",
    "pbar = tqdm(total=total_frames, desc=\"Generating Predictions\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (config.INPUT_WIDTH, config.INPUT_HEIGHT))\n",
    "    frame_buffer.append(resized_frame)\n",
    "    \n",
    "    if len(frame_buffer) == 3:\n",
    "        imgs_list = [torch.from_numpy(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)).float() / 255.0 for f in frame_buffer]\n",
    "        input_tensor = torch.cat([t.permute(2, 0, 1) for t in imgs_list], dim=0).unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        \n",
    "        pred_heatmap = torch.argmax(predictions.squeeze(0), dim=0).cpu().numpy()\n",
    "        # Get center_x, center_y, and radius from our modified postprocess function\n",
    "        px, py, pr = postprocess(pred_heatmap)\n",
    "        \n",
    "        if px is not None:\n",
    "            # Scale coordinates back to original frame size\n",
    "            scale_x = original_width / config.INPUT_WIDTH\n",
    "            scale_y = original_height / config.INPUT_HEIGHT\n",
    "            \n",
    "            center_x = px * scale_x\n",
    "            center_y = py * scale_y\n",
    "            # We can average the scaling factors for the radius\n",
    "            radius = pr * ((scale_x + scale_y) / 2.0)\n",
    "            \n",
    "            # Convert to bounding box format\n",
    "            tracknet_x = center_x - radius\n",
    "            tracknet_y = center_y - radius\n",
    "            tracknet_w = 20\n",
    "            tracknet_h = 20\n",
    "            \n",
    "            predictions_list.append({\n",
    "                'frame_id': frame_id_counter,\n",
    "                'tracknet_x': int(tracknet_x),\n",
    "                'tracknet_y': int(tracknet_y),\n",
    "                'tracknet_w': int(tracknet_w),\n",
    "                'tracknet_h': int(tracknet_h)\n",
    "            })\n",
    "        else:\n",
    "            # If no ball is detected, record NaNs for the coordinates\n",
    "            predictions_list.append({\n",
    "                'frame_id': frame_id_counter,\n",
    "                'tracknet_x': np.nan,\n",
    "                'tracknet_y': np.nan,\n",
    "                'tracknet_w': np.nan,\n",
    "                'tracknet_h': np.nan\n",
    "            })\n",
    "            \n",
    "    frame_id_counter += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "# Release resources\n",
    "pbar.close()\n",
    "cap.release()\n",
    "\n",
    "# --- Create and save the DataFrame ---\n",
    "tracknet_df = pd.DataFrame(predictions_list)\n",
    "tracknet_df.to_csv(config.OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"\\n--- Prediction Generation Complete ---\")\n",
    "print(f\"Predictions saved to: {config.OUTPUT_CSV_PATH}\")\n",
    "print(\"\\nFirst 10 rows of the prediction data:\")\n",
    "print(tracknet_df.head(10))\n",
    "\n",
    "# Print summary of detections\n",
    "detected_frames = tracknet_df['tracknet_x'].notna().sum()\n",
    "print(f\"\\nDetected a ball in {detected_frames} out of {total_frames} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3cf3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: 207 frames, 30 fps, 1280x720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|██████████| 207/207 [00:06<00:00, 30.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Trajectory video saved to: videos\\Clip1_tracknet_trajectory.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(config.VIDEO_CLIP_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file at {config.VIDEO_CLIP_PATH}\")\n",
    "    assert False\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# *** FIXED: Use OUTPUT_TRACKING_VIDEO_PATH for the trajectory video ***\n",
    "video_writer = cv2.VideoWriter(str(config.OUTPUT_TRACKING_VIDEO_PATH), fourcc, fps, (original_width, original_height))\n",
    "\n",
    "frame_buffer = collections.deque(maxlen=3)\n",
    "# *** NEW: List to store all detected ball coordinates ***\n",
    "trajectory_points = []\n",
    "\n",
    "print(f\"\\nProcessing video: {total_frames} frames, {fps} fps, {original_width}x{original_height}\")\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (config.INPUT_WIDTH, config.INPUT_HEIGHT))\n",
    "    frame_buffer.append(resized_frame)\n",
    "    \n",
    "    current_ball_pos = None # Reset current position for this frame\n",
    "    if len(frame_buffer) == 3:\n",
    "        imgs_list = [torch.from_numpy(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)).float() / 255.0 for f in frame_buffer]\n",
    "        input_tensor = torch.cat([t.permute(2, 0, 1) for t in imgs_list], dim=0).unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        \n",
    "        pred_heatmap = torch.argmax(predictions.squeeze(0), dim=0).cpu().numpy()\n",
    "        px, py = postprocess(pred_heatmap)\n",
    "        \n",
    "        if px is not None:\n",
    "            original_x = int(px * (original_width / config.INPUT_WIDTH))\n",
    "            original_y = int(py * (original_height / config.INPUT_HEIGHT))\n",
    "            current_ball_pos = (original_x, original_y)\n",
    "            # *** NEW: Add the new point to our history ***\n",
    "            trajectory_points.append(current_ball_pos)\n",
    "            \n",
    "    # *** NEW: Draw the entire trajectory on the current frame ***\n",
    "    if len(trajectory_points) > 1:\n",
    "        # Loop from the second point to the end to draw lines between consecutive points\n",
    "        for i in range(1, len(trajectory_points)):\n",
    "            cv2.line(frame, trajectory_points[i - 1], trajectory_points[i], config.TRAIL_COLOR, config.TRAIL_THICKNESS)\n",
    "            \n",
    "    # Draw the circle for the current frame's detected position (if any)\n",
    "    if current_ball_pos is not None:\n",
    "        cv2.circle(frame, current_ball_pos, config.CIRCLE_RADIUS, config.CIRCLE_COLOR, config.CIRCLE_THICKNESS)\n",
    "\n",
    "    video_writer.write(frame)\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "print(f\"\\n--- Inference Complete ---\")\n",
    "print(f\"Trajectory video saved to: {config.OUTPUT_TRACKING_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
